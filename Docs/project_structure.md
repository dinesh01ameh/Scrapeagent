# SwissKnife AI Scraper - Project Structure

## ğŸ“ Directory Structure

```
SwissKnife AI Scraper/
â”œâ”€â”€ ğŸ“„ README.md                    # Project documentation
â”œâ”€â”€ ğŸ“„ requirements.txt             # Python dependencies
â”œâ”€â”€ ğŸ“„ pyproject.toml              # Modern Python project configuration
â”œâ”€â”€ ğŸ“„ .env.example                # Environment variables template
â”œâ”€â”€ ğŸ“„ .gitignore                  # Git ignore rules
â”œâ”€â”€ ğŸ“„ Dockerfile                  # Docker container configuration
â”œâ”€â”€ ğŸ“„ docker-compose.yml          # Multi-service Docker setup
â”œâ”€â”€ ğŸ“„ Makefile                    # Common development tasks
â”œâ”€â”€ ğŸ“„ main.py                     # FastAPI application entry point
â”œâ”€â”€ ğŸ“„ PORT_MANAGEMENT.md          # Port management documentation
â”‚
â”œâ”€â”€ ğŸ“ config/                     # Configuration management
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ settings.py                # Pydantic settings with env support
â”‚
â”œâ”€â”€ ğŸ“ core/                       # Core business logic
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ scraper.py                 # Main SwissKnife scraper class
â”‚
â”œâ”€â”€ ğŸ“ api/                        # API layer
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ routes/                    # API route definitions
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ health.py              # Health check endpoints
â”‚       â”œâ”€â”€ scraping.py            # Main scraping endpoints
â”‚       â””â”€â”€ admin.py               # Admin and management endpoints
â”‚
â”œâ”€â”€ ğŸ“ features/                   # Core feature implementations
â”‚   â”œâ”€â”€ adaptive_extraction.py     # âœ… Intelligent extraction strategies
â”‚   â”œâ”€â”€ local_llm_integration.py   # âœ… Ollama LLM integration
â”‚   â”œâ”€â”€ natural_language_interface.py  # ğŸ”„ NLP query processing
â”‚   â”œâ”€â”€ proxy_rotation.py          # ğŸ”„ Advanced proxy management
â”‚   â””â”€â”€ multimodal_processing.py   # ğŸ”„ Multi-content processing
â”‚
â”œâ”€â”€ ğŸ“ models/                     # Data models and schemas
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ schemas.py                 # Pydantic models for API
â”‚
â”œâ”€â”€ ğŸ“ services/                   # Business services
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ ğŸ“ utils/                      # Utility functions
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ exceptions.py              # Custom exceptions and handlers
â”‚   â”œâ”€â”€ logging.py                 # Logging configuration
â”‚   â””â”€â”€ port_manager.py            # Port management utilities
â”‚
â”œâ”€â”€ ğŸ“ tests/                      # Test suite
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ test_main.py               # Basic application tests
â”‚
â”œâ”€â”€ ğŸ“ scripts/                    # Utility scripts
â”‚   â”œâ”€â”€ start.py                   # Application startup script
â”‚   â””â”€â”€ port_check.py              # Port management script
â”‚
â”œâ”€â”€ ğŸ“ swiss_knife/                # Legacy directory (to be integrated)
â”‚   â””â”€â”€ day_one_core.py
â”‚
â””â”€â”€ ğŸ“ Docs/                       # Documentation
    â”œâ”€â”€ PRD.md                     # Product Requirements Document
    â”œâ”€â”€ tech_stack.md              # Technology stack overview
    â”œâ”€â”€ Features.md
    â”œâ”€â”€ Smart_Scraper_AI.md
    â”œâ”€â”€ crawl4ai_custom_context.md
    â”œâ”€â”€ jin.md
    â”œâ”€â”€ project_structure.md       # This file
    â””â”€â”€ project_status.md          # Development status log
```

## ğŸ—ï¸ Architecture Overview

### Core Components

1. **FastAPI Application (`main.py`)**
   - Application entry point and lifecycle management
   - Middleware configuration (CORS, error handling)
   - Route registration and dependency injection

2. **Configuration Management (`config/`)**
   - Environment-based settings with Pydantic
   - Feature flags and service configuration
   - Automatic directory creation

3. **Core Scraper (`core/scraper.py`)**
   - Main orchestration class
   - Component initialization and management
   - Session handling and state tracking

4. **Feature Implementations (`features/`)**
   - **Adaptive Extraction**: âœ… Intelligent strategy selection with CSS, XPath, Regex, and LLM fallbacks
   - **Local LLM Integration**: âœ… Complete Ollama integration with model management
   - **Natural Language Interface**: ğŸ”„ Needs implementation
   - **Proxy Rotation**: ğŸ”„ Needs implementation  
   - **Multimodal Processing**: ğŸ”„ Needs implementation

5. **API Layer (`api/routes/`)**
   - RESTful endpoints for all functionality
   - Request/response models with validation
   - Error handling and status reporting

## ğŸš€ Getting Started

### Prerequisites
- Python 3.11+
- Docker & Docker Compose
- Ollama (for local LLM)

### Quick Start
```bash
# 1. Clone and setup
git clone <repository>
cd scrapeagent

# 2. Setup environment
make setup

# 3. Start services
make docker-up

# 4. Start application
make start
```

### Manual Setup
```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Setup environment
cp .env.example .env
# Edit .env with your configuration

# 3. Install Ollama models
ollama pull mistral
ollama pull llama3.2

# 4. Start application
python scripts/start.py
```

## ğŸ”§ Development

### Available Commands
```bash
make help          # Show all available commands
make install       # Install dependencies
make dev           # Setup development environment
make start         # Start application on port 8601
make restart-clean # Clean restart (kill old processes)
make test          # Run tests
make lint          # Run code linting
make format        # Format code
make check-port    # Check port 8601 status
make kill-port     # Kill processes on port 8601
make docker-up     # Start with Docker
```

### Testing
```bash
# Run all tests
make test

# Run with coverage
make test-cov

# Run specific test
pytest tests/test_main.py -v
```

## ğŸ“Š Current Implementation Status

### âœ… Completed
- [x] Project structure and configuration
- [x] FastAPI application with proper middleware
- [x] Configuration management with environment variables
- [x] Docker and Docker Compose setup
- [x] Adaptive extraction engine with multiple strategies
- [x] Local LLM integration with Ollama
- [x] API endpoints for health checks and basic scraping
- [x] Error handling and logging system
- [x] Port management system with automatic conflict resolution
- [x] Basic test structure
- [x] Development tooling (Makefile, startup scripts)

### ğŸ”„ In Progress / TODO
- [ ] Complete natural language interface implementation
- [ ] Advanced proxy rotation system
- [ ] Multimodal content processing (images, PDFs, etc.)
- [ ] Database integration (Supabase/PostgreSQL)
- [ ] Session management and user authentication
- [ ] Batch processing and job queues
- [ ] Monitoring and metrics (Prometheus/Grafana)
- [ ] Comprehensive test suite
- [ ] Documentation and tutorials

### ğŸ¯ Next Steps
1. **Complete Core Features**: Finish implementing the remaining feature modules
2. **Database Integration**: Set up Supabase or PostgreSQL for data persistence
3. **Enhanced Testing**: Add comprehensive unit and integration tests
4. **Performance Optimization**: Implement caching and optimize extraction strategies
5. **User Interface**: Build a web interface for easier interaction
6. **Documentation**: Create comprehensive API documentation and user guides

## ğŸ”— Key Endpoints

Once running, the application provides:

- **API Documentation**: http://localhost:8601/docs
- **Health Check**: http://localhost:8601/health
- **Basic Scraping**: POST http://localhost:8601/api/v1/scrape/
- **Natural Language**: POST http://localhost:8601/api/v1/scrape/natural-language
- **Admin Panel**: http://localhost:8601/api/v1/admin/status
- **Port Management**: GET http://localhost:8601/api/v1/admin/port/status

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run tests: `make test`
5. Format code: `make format`
6. Submit a pull request

## ğŸ“ Notes

- The project uses modern Python practices with Pydantic, FastAPI, and async/await
- All AI processing happens locally using Ollama for privacy
- The architecture is designed for scalability and modularity
- Docker support enables easy deployment and development
- Port 8601 is permanently assigned with automatic conflict resolution
